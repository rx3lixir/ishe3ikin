package scraper

import (
	"context"
	"errors"
	"fmt"
	"strings"
	"sync"

	"github.com/charmbracelet/log"
	"github.com/go-rod/rod"
	"github.com/go-rod/stealth"
	"github.com/rx3lixir/ish3ikin/internal/config/taskconfig"
)

type RodScraper struct {
	Tasks   taskconfig.TaskConfig
	Browser *rod.Browser
	Logger  *log.Logger
}

// NewRodScraper —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π RodScraper.
func NewRodScraper(browser *rod.Browser, config taskconfig.TaskConfig, logger *log.Logger) *RodScraper {
	return &RodScraper{
		Tasks:   config,
		Browser: browser,
		Logger:  logger,
	}
}

// Scraper –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞.
type Scraper interface {
	Scrape(ctx context.Context) (map[string]string, error)
}

func (r *RodScraper) Scrape(ctx context.Context) (map[string]string, error) {
	r.Logger.Info("üåê Starting scraping", "url:", r.Tasks.URL)

	// –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º stealth
	page, err := stealth.Page(r.Browser)
	if err != nil {
		return nil, fmt.Errorf("failed to create page: %v", r.Tasks.URL)
	}

	// –ù–∞–≤–∏–≥–∞—Ü–∏—è –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π URL
	err = page.Navigate(r.Tasks.URL)
	if err != nil {
		return nil, fmt.Errorf("failed to navigate to page: %v", r.Tasks.URL)
	}

	// –û–∂–∏–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
	err = page.WaitLoad()
	if err != nil {
		r.Logger.Warn("‚≠ï Page did not load fully", "url:", r.Tasks.URL, "error:", err)
	}

	// –°–æ–∑–¥–∞—ë–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
	results := make(map[string]string)

	// –ü—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
	for key, selector := range r.Tasks.Selectors {
		// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
		if selector == "" {
			results[key] = ""
			continue
		}

		// –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É
		elements, err := page.Elements(selector)
		if err != nil || len(elements) == 0 {
			r.Logger.Warn("‚≠ï No elements found", "selector:", selector, "error:", err)
			results[key] = ""
			continue
		}

		// –°–±–æ—Ä —Ç–µ–∫—Å—Ç–∞ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
		var texts []string
		for _, element := range elements {
			text, err := element.Text()
			if err != nil {
				r.Logger.Warn("‚≠ï Failed to get text for element", "selector:", selector, "error:", err)
				continue
			}
			texts = append(texts, text)
		}

		// –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏)
		results[key] = strings.Join(texts, "\n")
		r.Logger.Info("‚úÖ Successfully scraped", "key:", key, "count:", len(texts))
	}

	// –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
	return results, nil
}

// TaskRunner —É–ø—Ä–∞–≤–ª—è–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∑–∞–¥–∞—á.
type TaskRunner struct {
	Scrapers []Scraper
}

func NewTaskRunner(scrapers []Scraper) *TaskRunner {
	return &TaskRunner{Scrapers: scrapers}
}

func (t *TaskRunner) Run(ctx context.Context) ([]map[string]string, []error) {
	var wg sync.WaitGroup
	resultsChan := make(chan map[string]string)
	errorsChan := make(chan error)

	for _, scraper := range t.Scrapers {
		wg.Add(1)
		go func(s Scraper) {
			defer wg.Done()
			result, err := s.Scrape(ctx)
			if err != nil {
				errorsChan <- err
				return
			}
			resultsChan <- result
		}(scraper)
	}

	go func() {
		wg.Wait()
		close(resultsChan)
		close(errorsChan)
	}()

	var results []map[string]string
	var errorList []error

	for {
		select {
		case res, ok := <-resultsChan:
			if !ok {
				resultsChan = nil
			} else {
				results = append(results, res)
			}
		case err, ok := <-errorsChan:
			if !ok {
				errorsChan = nil
			} else {
				errorList = append(errorList, err)
			}
		case <-ctx.Done():
			errorList = append(errorList, errors.New("scraping tasks timed out"))
			return results, errorList
		}

		if resultsChan == nil && errorsChan == nil {
			break
		}
	}

	return results, errorList
}
